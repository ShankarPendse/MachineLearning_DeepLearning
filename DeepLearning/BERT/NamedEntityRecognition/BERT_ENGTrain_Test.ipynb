{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import more_itertools as mit\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "# Maximmum sequence length (we know from data exploration, longest sentence has 76 tokens in it)\n",
    "MAX_LEN = 80\n",
    "\n",
    "# Training batch size\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "\n",
    "# Validation batch size\n",
    "VAL_BATCH_SIZE = 4\n",
    "\n",
    "# Number of epochs to train the model\n",
    "EPOCHS = 10\n",
    "\n",
    "# Model path to use the pre trained model\n",
    "BASE_MODEL_PATH = \"./BERT/bert_base_uncase\"\n",
    "\n",
    "# Path to save the trained model\n",
    "MODEL_PATH = \"model_BERT_ENG.bin\"\n",
    "\n",
    "# File to read the data for tarining purpose\n",
    "TRAINING_FILE = \"./data/engtrain.bio\"\n",
    "\n",
    "# Bert Tokenizer\n",
    "TOKENIZER = BertTokenizer.from_pretrained(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set class whose object will be passed as an argument to generator to generate the batches of data set for training and validation purpose\n",
    "\n",
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "    \n",
    "    # invoked when len() is called upon the generator\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    # called when a generator gets invoked\n",
    "    def __getitem__(self, ind):\n",
    "        text = self.texts[ind]\n",
    "        tags = self.tags[ind]\n",
    "        \n",
    "        # variables to hold the encoded sequences and tag ids (lable for our supervised trainning)\n",
    "        ids = []\n",
    "        target_tags = []\n",
    "        \n",
    "        # gives us each word in a sentence\n",
    "        for i, seq in enumerate(text):\n",
    "            # Tokenizing and encoding the input sequence (each word) as per BERT tokenizer without [SEP] and [CLS] as we are assigning it manually\n",
    "            inputs = TOKENIZER.encode(seq, add_special_tokens=False)\n",
    "            \n",
    "            # no of sub words the 'seq' is split into\n",
    "            input_len = len(inputs)\n",
    "            \n",
    "            # storing token ids in a list\n",
    "            ids.extend(inputs)\n",
    "            \n",
    "            # for each token length of target lable must be same as the length of token ids generated by tokenizer\n",
    "            target_tags.extend([tags[i]] * input_len)\n",
    "\n",
    "        # Now that we have parsed one sentence (token wise in the above for loop), we can add special tokens [CLS] and [SEP] at the beginning and at the end of the sequence\n",
    "        ids = [101] + ids + [102]\n",
    "        \n",
    "        # make target tags to have the same length as ids\n",
    "        target_tags = [0] + target_tags + [0]\n",
    "\n",
    "        # Attention mask, setting 1 will let the model know to use those (attend those tokens)\n",
    "        mask = [1] * len(ids)\n",
    "        \n",
    "        # token_type_ids will be set to 0 for all tokens as we are not dealing with sentence pairs (for task like eg: QnA)\n",
    "        token_type_ids = [0] *len(ids)\n",
    "\n",
    "        # Calculating how much padding is required for a sequence. Since, ids and target tags are all of same length, we can use any one to calculate the required padding length\n",
    "        padding_len = MAX_LEN- len(ids)\n",
    "\n",
    "        # pad the ids with 0\n",
    "        ids = ids + ([0]*padding_len)\n",
    "        \n",
    "        # mask the padded tokens with 0 (to let the model know to ignore these tokens)\n",
    "        mask = mask + ([0]*padding_len)\n",
    "        \n",
    "        # setting token type ids of padded tokens to 0 (we are not actually using these in this task)\n",
    "        token_type_ids = token_type_ids + ([0]*padding_len)\n",
    "        \n",
    "        # padding the target tags\n",
    "        target_tags = target_tags + ([0]*padding_len)\n",
    "        \n",
    "        # returning dictionary of ids, mask, token_type_ids and target_tags. all will have same length (128 which is set as MAX_LEN)\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tags\": torch.tensor(target_tags, dtype=torch.long)\n",
    "        }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    # setting the model in training mode, so that we can update the gradients\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    # data_loader will be of train_dataset, with data loader generator for each call, dataset equal to batch size will be supplied to pass through the model\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        # accessing ids, attention mask, token_type_ids and target tags as returned by __getitem__\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        \n",
    "        # Setting gradients of the model parameters to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pushing the data through the network and saving the loss for this pass\n",
    "        _, loss = model.forward(**data)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters with the calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Takes care of learning rate decay\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "    \n",
    "    return final_loss / len(data_loader)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model after updating the model gradients with one batch of data\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    # setting the model in evaluation mode, so that we can use the last updated gradients to predict\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            # This will transfer the data to GPU\n",
    "            data[k] = v.to(device)\n",
    "        _, loss = model.forward(**data)\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the loss (cross entropy loss)\n",
    "\n",
    "def loss_fn(output, target, mask, num_labels):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(active_loss, target.view(-1), torch.tensor(lfn.ignore_index).type_as(target))\n",
    "    \n",
    "    loss = lfn(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create the model\n",
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag):\n",
    "        super(EntityModel, self).__init__()\n",
    "        \n",
    "        # Defining the model architecture\n",
    "        self.bert = transformers.BertModel.from_pretrained(BASE_MODEL_PATH)\n",
    "        self.num_tag = num_tag\n",
    "        self.bert_drop_1 = nn.Dropout(0.5)\n",
    "        \n",
    "        # output dimension will be 768Xnum_tag. 768 because we are using bert_base_uncase\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "        \n",
    "    # Forward pass method to pass the data through the network\n",
    "    def forward(self, ids, mask, token_type_ids, target_tags):\n",
    "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        bo_tag = self.bert_drop_1(o1)\n",
    "        \n",
    "        tag = self.out_tag(bo_tag)\n",
    "        \n",
    "        loss = loss_fn(tag, target_tags, mask, self.num_tag)\n",
    "\n",
    "        return tag, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process_data(path):\n",
    "    \n",
    "    # Read the data file, sentences are represented as words and each sentence is separated by a blank line\n",
    "    df = pd.read_csv(path, sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=False, dtype=\"string\", skipfooter=1)\n",
    "    \n",
    "    # fill the blank line values with some value, to construct sentences from word \n",
    "    df.fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    filt = df['TAG']!=\"split_at\"\n",
    "    \n",
    "    # Using label encoder to encode the target tags\n",
    "    tags_enc = LabelEncoder()\n",
    "    tags_enc.fit_transform(df.loc[filt, 'TAG'])\n",
    "    dic = dict(zip(tags_enc.classes_, tags_enc.transform(tags_enc.classes_)))\n",
    "    \n",
    "    df['TAG'] = df['TAG'].map(dic)\n",
    "    df['TAG'].fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    # Constructing the sentences from word representation\n",
    "    sentences = np.array(list(mit.split_at(df['WORD'].tolist(), pred=lambda x: x=='split_at')))\n",
    "    \n",
    "    # Constructing tags for each sentence as a whole from each word\n",
    "    tags = np.array(list(mit.split_at(df['TAG'].tolist(), pred=lambda x: x=='split_at' )))\n",
    "    \n",
    "    return sentences, tags, tags_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "Some weights of the model checkpoint at ./BERT/bert_base_uncase were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.543764461864802 Valid Loss = 0.24740117401340786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.2030286230637946 Valid Loss = 0.20728233872665738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.15633766272697935 Valid Loss = 0.20296533088180788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.13010746813040566 Valid Loss = 0.19662809569521675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.71it/s]\n",
      "  0%|                                                                                         | 0/1100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.11027436334639788 Valid Loss = 0.20189963285716211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.70it/s]\n",
      "  0%|                                                                                         | 0/1100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.09482114355418493 Valid Loss = 0.2070213700644672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.72it/s]\n",
      "  0%|                                                                                         | 0/1100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0830725264900618 Valid Loss = 0.20710446471966118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.73it/s]\n",
      "  0%|                                                                                         | 0/1100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0732365293237804 Valid Loss = 0.212727547721101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.68it/s]\n",
      "  0%|                                                                                         | 0/1100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.06629291739653458 Valid Loss = 0.21592167988229466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1100/1100 [05:20<00:00,  3.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 245/245 [00:13<00:00, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.06271925185637718 Valid Loss = 0.21926759854101632\n",
      "Wall time: 55min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    file_path = \"./data/engtrain.bio\"\n",
    "    sentences, tags, tags_encoder = read_and_process_data(file_path)\n",
    "    \n",
    "    metadata = {\n",
    "        \"enc_tag\" : tags_encoder\n",
    "    }\n",
    "    \n",
    "    joblib.dump(metadata, \"meta.bin\")\n",
    "    \n",
    "    num_tags = len(tags_encoder.classes_)\n",
    "    \n",
    "    # Split the data into train and validation set using train_test_split with 10% as val data\n",
    "    (train_sentences, test_sentences, train_tag, test_tag) = train_test_split(sentences, tags, random_state=43, test_size=0.1)\n",
    "    \n",
    "    # dataset object for train data loader\n",
    "    train_dataset = EntityDataset(\n",
    "        texts=train_sentences, tags=train_tag\n",
    "    )\n",
    "\n",
    "    # Generator object which will be used for model training purpose\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=0\n",
    "    )\n",
    "\n",
    "    # dataset object for validation data loader\n",
    "    valid_dataset = EntityDataset(\n",
    "        texts=test_sentences, tags=test_tag\n",
    "    )\n",
    "\n",
    "    # Generator object which will be used for model evaluation purpose after processing each batch from train data loader\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VAL_BATCH_SIZE, num_workers=0\n",
    "    )\n",
    "\n",
    "    # This will transfer the model to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = EntityModel(num_tags)\n",
    "    model.to(device)\n",
    "\n",
    "    # Getting the parameters of the model to be passed to the optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    \n",
    "    # Not decaying the weights for bias, LyerNorm weight and bias\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    \n",
    "    # Getting the list of parameters to use in decay\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Calculating the total number of training steps required to pass to the linear scheduler for decreasing the learning rate\n",
    "    num_train_steps = int(len(train_sentences) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    \n",
    "    # Adam optimizer with weight decay (regularizing the variables/parameters with large gradients)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=1e-5)\n",
    "    \n",
    "    # scheduler to linearly decay the learning rate, setting num_warmup_steps to 0, helps in initial learning rate to be at the specified value\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # Train the model for EPOCHS number of times\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        test_loss = eval_fn(valid_data_loader, model, device)\n",
    "        print(f\"Train loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "        if  test_loss < best_loss:\n",
    "            # Saving the weights if loss is better than previous epoch loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to prepare the test data set supplied to us, in the same way we prepared train dataset, but we will use tags_enc transform method to transform the tags\n",
    "\n",
    "\n",
    "def prepare_test_data(path, tags_enc):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=False, dtype=\"string\", skipfooter=1)\n",
    "    df.fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    filt = df['TAG']!=\"split_at\"\n",
    "    tags_enc.transform(df.loc[filt, 'TAG'])\n",
    "    dic = dict(zip(tags_enc.classes_, tags_enc.transform(tags_enc.classes_)))\n",
    "    \n",
    "    df['TAG'] = df['TAG'].map(dic)\n",
    "    df['TAG'].fillna(\"split_at\", inplace=True)\n",
    "        \n",
    "    sentences = np.array(list(mit.split_at(df['WORD'].tolist(), pred=lambda x: x=='split_at')))\n",
    "    tags = np.array(list(mit.split_at(df['TAG'].tolist(), pred=lambda x: x=='split_at' )))\n",
    "    \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, device, model):   \n",
    "    all_tags_prob = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "            tags_prob, _ = model.forward(**data)\n",
    "            all_tags_prob.extend(tags_prob)\n",
    "    \n",
    "    return all_tags_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions(data_path):\n",
    "    \n",
    "    metadata = joblib.load(\"meta.bin\")\n",
    "    tags_enc = metadata['enc_tag']\n",
    "    num_tags = len(tags_enc.classes_)\n",
    "    \n",
    "    test_sentences, test_tags= prepare_test_data(path=data_path, tags_enc=tags_enc)\n",
    "    \n",
    "    model = EntityModel(num_tag=num_tags)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    \n",
    "    test_dataset = EntityDataset(\n",
    "        texts=test_sentences, tags=test_tags\n",
    "    )\n",
    "    \n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=VAL_BATCH_SIZE, num_workers=0\n",
    "    )\n",
    "    \n",
    "    predicted_tags_prob = predict(test_data_loader, device, model)\n",
    "    return test_tags, predicted_tags_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(test_tags, predicted_tags_prob):\n",
    "    predicted_tags = []\n",
    "    for i in range(len(test_tags)):\n",
    "        # for each test sentence get the predicted tag value using argmax and upto the length of actual sentence (not the padded sentence)\n",
    "        predicted_tags.append(predicted_tags_prob[i].cpu().numpy().argmax(1).reshape(-1)[1:len(test_tags[i])+1])\n",
    "\n",
    "    print(classification_report(np.concatenate(test_tags), np.concatenate(predicted_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  app.launch_new_instance()\n",
      "Some weights of the model checkpoint at ./BERT/bert_base_uncase were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"./data/engtest.bio\"\n",
    "test_tags, predicted_tags_prob = test_predictions(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.86      0.79       812\n",
      "         1.0       0.48      0.64      0.55        90\n",
      "         2.0       0.78      0.81      0.79       456\n",
      "         3.0       0.78      0.80      0.79      1117\n",
      "         4.0       0.65      0.60      0.63       491\n",
      "         5.0       0.77      0.82      0.79       500\n",
      "         6.0       0.68      0.68      0.68       451\n",
      "         7.0       0.21      0.16      0.18        56\n",
      "         8.0       0.53      0.69      0.60        54\n",
      "         9.0       0.75      0.84      0.79       562\n",
      "        10.0       0.78      0.83      0.81        30\n",
      "        11.0       0.77      0.71      0.74       720\n",
      "        12.0       0.56      0.75      0.64       862\n",
      "        13.0       0.44      0.48      0.46        75\n",
      "        14.0       0.57      0.73      0.64       496\n",
      "        15.0       0.74      0.60      0.67       222\n",
      "        16.0       0.78      0.54      0.63       496\n",
      "        17.0       0.83      0.74      0.78       226\n",
      "        18.0       0.76      0.68      0.72       403\n",
      "        19.0       0.31      0.09      0.14        45\n",
      "        20.0       0.81      0.73      0.77       119\n",
      "        21.0       0.87      0.85      0.86       856\n",
      "        22.0       0.00      0.00      0.00         8\n",
      "        23.0       0.87      0.73      0.79       610\n",
      "        24.0       0.91      0.90      0.91     14929\n",
      "\n",
      "    accuracy                           0.84     24686\n",
      "   macro avg       0.65      0.65      0.65     24686\n",
      "weighted avg       0.84      0.84      0.84     24686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "measure_performance(test_tags, predicted_tags_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are getting a macro avg F1 Score of 0.65\n",
    "## Model is not performing great, it is failing completely for labes encoded as 7, 19 and 22\n",
    "\n",
    "## Possible improvements:\n",
    "###  Train model for large number of epochs\n",
    "###  Try using large bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
