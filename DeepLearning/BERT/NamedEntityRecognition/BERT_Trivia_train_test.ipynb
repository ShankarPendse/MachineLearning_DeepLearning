{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import more_itertools as mit\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximmum sequence length (we know from data exploration, longest sentence has 76 tokens in it)\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Training batch size\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "\n",
    "# Validation batch size\n",
    "VAL_BATCH_SIZE = 4\n",
    "\n",
    "# Number of epochs to train the model\n",
    "EPOCHS = 10\n",
    "\n",
    "# Model path to use the pre trained model\n",
    "BASE_MODEL_PATH = \"./BERT/bert_base_uncase\"\n",
    "\n",
    "# Path to save the trained model\n",
    "MODEL_PATH = \"model_trivia.bin\"\n",
    "\n",
    "# File to read the data for tarining purpose\n",
    "TRAINING_FILE = \"./data/trivia10k13train.bio\"\n",
    "\n",
    "# Bert Tokenizer\n",
    "TOKENIZER = BertTokenizer.from_pretrained(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set class whose object will be passed as an argument to generator to generate the batches of data set for training and validation purpose\n",
    "\n",
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        \n",
    "    # invoked when len() is called upon the generator\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    # called when a generator gets invoked\n",
    "    def __getitem__(self, ind):\n",
    "        text = self.texts[ind]\n",
    "        tags = self.tags[ind]\n",
    "        \n",
    "        # variables to hold the encoded sequences and tag ids (lable for our supervised trainning)\n",
    "        ids = []\n",
    "        target_tags = []\n",
    "        \n",
    "        # gives us each word in a sentence\n",
    "        for i, s in enumerate(text):\n",
    "            # Tokenizing and encoding the input sequence (each word) as per BERT tokenizer without [SEP] and [CLS] as we are assigning it manually\n",
    "            inputs = TOKENIZER.encode(s, add_special_tokens=False)\n",
    "            \n",
    "            # no of sub words the 'seq' is split into\n",
    "            input_len = len(inputs)\n",
    "            \n",
    "            # storing token ids in a list\n",
    "            ids.extend(inputs)\n",
    "            \n",
    "            # for each token length of target lable must be same as the length of token ids generated by tokenizer\n",
    "            target_tags.extend([tags[i]] * input_len)\n",
    "\n",
    "        # Now that we have parsed one sentence (token wise in the above for loop), we can add special tokens [CLS] and [SEP] at the beginning and at the end of the sequence\n",
    "        ids = [101] + ids + [102]\n",
    "        \n",
    "        # make target tags to have the same length as ids\n",
    "        target_tags = [0] + target_tags + [0]\n",
    "\n",
    "        # Attention mask, setting 1 will let the model know to use those (attend those tokens)\n",
    "        mask = [1] * len(ids)\n",
    "        \n",
    "        # token_type_ids will be set to 0 for all tokens as we are not dealing with sentence pairs (for task like eg: QnA)\n",
    "        token_type_ids = [0] *len(ids)\n",
    "\n",
    "        # Calculating how much padding is required for a sequence. Since, ids and target tags are all of same length, we can use any one to calculate the required padding length\n",
    "        padding_len = MAX_LEN- len(ids)\n",
    "\n",
    "        # pad the ids with 0\n",
    "        ids = ids + ([0]*padding_len)\n",
    "        \n",
    "        # mask the padded tokens with 0 (to let the model know to ignore these tokens)\n",
    "        mask = mask + ([0]*padding_len)\n",
    "        \n",
    "        # setting token type ids of padded tokens to 0 (we are not actually using these in this task)\n",
    "        token_type_ids = token_type_ids + ([0]*padding_len)\n",
    "        \n",
    "        # padding the target tags\n",
    "        target_tags = target_tags + ([0]*padding_len)\n",
    "        \n",
    "        # returning dictionary of ids, mask, token_type_ids and target_tags. all will have same length (128 which is set as MAX_LEN)\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tags\": torch.tensor(target_tags, dtype=torch.long)\n",
    "        }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    # setting the model in training mode, so that we can update the gradients\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    # data_loader will be of train_dataset, with data loader generator for each call, dataset equal to batch size will be supplied to pass through the model\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        # accessing ids, attention mask, token_type_ids and target tags as returned by __getitem__\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "            \n",
    "        # Setting gradients of the model parameters to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pushing the data through the network and saving the loss for this pass\n",
    "        _, loss = model.forward(**data)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters with the calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Takes care of learning rate decay\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model after updating the model gradients with one batch of data\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    # setting the model in evaluation mode, so that we can use the last updated gradients to predict\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            # This will transfer the data to GPU\n",
    "            data[k] = v.to(device)\n",
    "        _, loss = model.forward(**data)\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the loss (cross entropy loss)\n",
    "\n",
    "def loss_fn(output, target, mask, num_labels):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(active_loss, target.view(-1), torch.tensor(lfn.ignore_index).type_as(target))\n",
    "    \n",
    "    loss = lfn(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create the model\n",
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag):\n",
    "        super(EntityModel, self).__init__()\n",
    "        \n",
    "        # Defining the model architecture\n",
    "        self.bert = transformers.BertModel.from_pretrained(BASE_MODEL_PATH)\n",
    "        self.num_tag = num_tag\n",
    "        self.bert_drop_1 = nn.Dropout(0.5)\n",
    "        \n",
    "        # output dimension will be 768Xnum_tag. 768 because we are using bert_base_uncase\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "        \n",
    "    # Forward pass method to pass the data through the network\n",
    "    def forward(self, ids, mask, token_type_ids, target_tags):\n",
    "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        bo_tag = self.bert_drop_1(o1)\n",
    "        \n",
    "        tag = self.out_tag(bo_tag)\n",
    "        \n",
    "        loss = loss_fn(tag, target_tags, mask, self.num_tag)\n",
    "\n",
    "        return tag, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process_data(path):\n",
    "    \n",
    "    # Read the data file, sentences are represented as words and each sentence is separated by a blank line\n",
    "    df = pd.read_csv(path, sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=False, dtype=\"string\", skipfooter=1)\n",
    "    \n",
    "    # Data set has a missing word (a row has only tag, but no word, so dropping that particular row from the dataframe)\n",
    "    df.drop(set(df[df['WORD'].isnull()].index.tolist()) - set(df[df['TAG'].isnull()].index.tolist()), inplace = True)\n",
    "    \n",
    "    # fill the blank line values with some value, to construct sentences from word \n",
    "    df.fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    filt = df['TAG']!=\"split_at\"\n",
    "    tags_enc = LabelEncoder()\n",
    "    tags_enc.fit_transform(df.loc[filt, 'TAG'])\n",
    "    dic = dict(zip(tags_enc.classes_, tags_enc.transform(tags_enc.classes_)))\n",
    "    \n",
    "    df['TAG'] = df['TAG'].map(dic)\n",
    "    df['TAG'].fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    # Constructing the sentences from word representation\n",
    "    sentences = np.array(list(mit.split_at(df['WORD'].tolist(), pred=lambda x: x=='split_at')))\n",
    "    \n",
    "    # Constructing tags for each sentence as a whole from each word\n",
    "    tags = np.array(list(mit.split_at(df['TAG'].tolist(), pred=lambda x: x=='split_at' )))\n",
    "    \n",
    "    return sentences, tags, tags_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  app.launch_new_instance()\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "Some weights of the model checkpoint at ./BERT/bert_base_uncase were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.6767136649313298 Valid Loss = 0.385975470956491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.3399904260327193 Valid Loss = 0.34258458959128785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.2751714537428184 Valid Loss = 0.3337758387610012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.47it/s]\n",
      "  0%|                                                                                          | 0/880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.23363087477839806 Valid Loss = 0.3396248898126793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.45it/s]\n",
      "  0%|                                                                                          | 0/880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.20371354916979642 Valid Loss = 0.34960496395218127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.48it/s]\n",
      "  0%|                                                                                          | 0/880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.18235527528873222 Valid Loss = 0.36474053919961563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.45it/s]\n",
      "  0%|                                                                                          | 0/880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1651040407998318 Valid Loss = 0.3761894316887673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.46it/s]\n",
      "  0%|                                                                                          | 0/880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1520886292173104 Valid Loss = 0.3859476091682303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.46it/s]\n",
      "  0%|                                                                                          | 0/880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.14209967558483846 Valid Loss = 0.3939837696486894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 880/880 [05:50<00:00,  2.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:15<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.13629216216504575 Valid Loss = 0.3948139851266632\n",
      "Wall time: 1h 1min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    file_path = TRAINING_FILE\n",
    "    sentences, tags, tags_encoder = read_and_process_data(file_path)\n",
    "    \n",
    "    metadata = {\n",
    "        \"enc_tag\" : tags_encoder\n",
    "    }\n",
    "    \n",
    "    joblib.dump(metadata, \"meta.bin\")\n",
    "    \n",
    "    num_tags = len(tags_encoder.classes_)\n",
    "    \n",
    "    # Split the data into train and validation set using train_test_split with 10% as val data\n",
    "    (train_sentences, test_sentences, train_tag, test_tag) = train_test_split(sentences, tags, random_state=43, test_size=0.1)\n",
    "    \n",
    "    # dataset object for train data loader\n",
    "    train_dataset = EntityDataset(\n",
    "        texts=train_sentences, tags=train_tag\n",
    "    )\n",
    "\n",
    "    # Generator object which will be used for model training purpose\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=0\n",
    "    )\n",
    "\n",
    "    # dataset object for validation data loader\n",
    "    valid_dataset = EntityDataset(\n",
    "        texts=test_sentences, tags=test_tag\n",
    "    )\n",
    "\n",
    "    # Generator object which will be used for model evaluation purpose after processing each batch from train data loader\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VAL_BATCH_SIZE, num_workers=0\n",
    "    )\n",
    "\n",
    "    # This will transfer the model to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = EntityModel(num_tags)\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Calculating the total number of training steps required to pass to the linear schedular for decreasing the learning rate\n",
    "    num_train_steps = int(len(train_sentences) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    \n",
    "    # Adam optimizer with weight decay (regularizing the variables/parameters with large gradients)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=1e-5)\n",
    "    \n",
    "    # scheduler to linearly decay the learning rate, setting num_warmup_steps to 0, helps in initial learning rate to be at the specified value\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # Train the model for EPOCHS number of times\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        test_loss = eval_fn(valid_data_loader, model, device)\n",
    "        print(f\"Train loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "        if  test_loss < best_loss:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to prepare the test data set supplied to us, in the same way we prepared train dataset, but we will use tags_enc transform method to transform the tags\n",
    "\n",
    "\n",
    "def prepare_test_data(path, tags_enc):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=False, dtype=\"string\", skipfooter=1)\n",
    "    df.fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    filt = df['TAG']!=\"split_at\"\n",
    "    tags_enc.transform(df.loc[filt, 'TAG'])\n",
    "    dic = dict(zip(tags_enc.classes_, tags_enc.transform(tags_enc.classes_)))\n",
    "    \n",
    "    df['TAG'] = df['TAG'].map(dic)\n",
    "    df['TAG'].fillna(\"split_at\", inplace=True)\n",
    "        \n",
    "    sentences = np.array(list(mit.split_at(df['WORD'].tolist(), pred=lambda x: x=='split_at')))\n",
    "    tags = np.array(list(mit.split_at(df['TAG'].tolist(), pred=lambda x: x=='split_at' )))\n",
    "    \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, device, model):   \n",
    "    all_tags_prob = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "            tags_prob, _ = model.forward(**data)\n",
    "            all_tags_prob.extend(tags_prob)\n",
    "    \n",
    "    return all_tags_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions(data_path):\n",
    "    \n",
    "    metadata = joblib.load(\"meta.bin\")\n",
    "    tags_enc = metadata['enc_tag']\n",
    "    num_tags = len(tags_enc.classes_)\n",
    "    \n",
    "    test_sentences, test_tags= prepare_test_data(path=data_path, tags_enc=tags_enc)\n",
    "    \n",
    "    model = EntityModel(num_tag=num_tags)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    \n",
    "    test_dataset = EntityDataset(\n",
    "        texts=test_sentences, tags=test_tags\n",
    "    )\n",
    "    \n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=VAL_BATCH_SIZE, num_workers=0\n",
    "    )\n",
    "    \n",
    "    predicted_tags_prob = predict(test_data_loader, device, model)\n",
    "    return test_tags, predicted_tags_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(test_tags, predicted_tags_prob):\n",
    "    predicted_tags = []\n",
    "    for i in range(len(test_tags)):\n",
    "         # for each test sentence get the predicted tag value using argmax and upto the length of actual sentence (not the padded sentence)\n",
    "        predicted_tags.append(predicted_tags_prob[i].cpu().numpy().argmax(1).reshape(-1)[1:len(test_tags[i])+1])\n",
    "\n",
    "    print(classification_report(np.concatenate(test_tags), np.concatenate(predicted_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if sys.path[0] == '':\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  del sys.path[0]\n",
      "Some weights of the model checkpoint at ./BERT/bert_base_uncase were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"./data/trivia10k13test.bio\"\n",
    "test_tags, predicted_tags_prob = test_predictions(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.68      0.65      1274\n",
      "         1.0       0.47      0.33      0.39        66\n",
      "         2.0       0.36      0.46      0.40       283\n",
      "         3.0       0.68      0.81      0.74       425\n",
      "         4.0       0.67      0.74      0.70       789\n",
      "         5.0       0.40      0.50      0.45       195\n",
      "         6.0       0.39      0.33      0.36       190\n",
      "         7.0       0.42      0.40      0.41      1577\n",
      "         8.0       0.00      0.00      0.00        47\n",
      "         9.0       0.74      0.59      0.66       171\n",
      "        10.0       0.00      0.00      0.00         8\n",
      "        11.0       0.86      0.84      0.85       661\n",
      "        12.0       0.47      0.69      0.56      1553\n",
      "        13.0       0.62      0.69      0.65       147\n",
      "        14.0       0.32      0.38      0.35       227\n",
      "        15.0       0.51      0.78      0.62       411\n",
      "        16.0       0.73      0.67      0.70       544\n",
      "        17.0       0.48      0.08      0.13       143\n",
      "        18.0       0.64      0.78      0.70       808\n",
      "        19.0       0.93      0.88      0.90     14661\n",
      "        20.0       0.71      0.90      0.79       349\n",
      "        21.0       0.62      0.55      0.58       289\n",
      "        22.0       0.50      0.07      0.12        30\n",
      "        23.0       0.69      0.66      0.67        44\n",
      "        24.0       0.82      0.79      0.81     14143\n",
      "\n",
      "    accuracy                           0.78     39035\n",
      "   macro avg       0.55      0.54      0.53     39035\n",
      "weighted avg       0.80      0.78      0.79     39035\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "measure_performance(test_tags, predicted_tags_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared to LSTM, Though the accuracy is 78% (5% less than that of LSTM model) macro avg F1 score is better in this case and also we can see relatively better performance for the class labeled 1, 5, 9, 17, 21, 22, 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
