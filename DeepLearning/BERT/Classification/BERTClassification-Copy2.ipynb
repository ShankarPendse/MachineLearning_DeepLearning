{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeValues(encoder,df):\n",
    "    df_encoded = pd.DataFrame()\n",
    "    temp = pd.Series()\n",
    "    \n",
    "    df_encoded['ORT LEVEL 1'] = encoder.fit_transform(df['ORT Level 1'])\n",
    "    encoded_values = df_encoded['ORT LEVEL 1'].unique()\n",
    "    \n",
    "    for element in encoded_values:\n",
    "        temp = temp.append(pd.Series(encoder.fit_transform(df['ORT Level 2'].where(df_encoded['ORT LEVEL 1'] == element).dropna())),ignore_index=True)\n",
    "    \n",
    "    df_encoded['ORT LEVEL 2'] = temp\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictFunc(df,df_encoded):\n",
    "    ORT_LEVEL_1_DICT = dict(zip(df['ORT Level 1'].unique(),df_encoded['ORT LEVEL 1'].unique()))\n",
    "    \n",
    "    elements = df['ORT Level 2'].unique()\n",
    "    indices = []\n",
    "    \n",
    "    for element in elements:\n",
    "        indices.append(df[df['ORT Level 2'] == element].index[0])\n",
    "\n",
    "    ORT_LEVEL_2_DICT = dict(zip(df['ORT Level 2'].unique(),df_encoded['ORT LEVEL 2'].iloc[indices]))\n",
    "    \n",
    "    return ORT_LEVEL_1_DICT,ORT_LEVEL_2_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderFunc(df,level1_dict,level2_dict):\n",
    "    df['ORT LEVEL 1'] = df['ORT LEVEL 1'].map(level1_dict)\n",
    "    df['ORT LEVEL 2'] = df['ORT LEVEL 2'].map(level2_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,dataLoader):\n",
    "    train_loss_set = []\n",
    "\n",
    "    # Number of training epochs\n",
    "    epochs = 4\n",
    "\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "      # Set our model to training mode\n",
    "      model.train()\n",
    "  \n",
    "      # Tracking variables\n",
    "      tr_loss = 0\n",
    "      nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "      # Train the data for one epoch\n",
    "    for step, batch in enumerate(dataLoader):\n",
    "        b_input_ids, b_labels = batch\n",
    "        # Clear out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        loss = model(b_input_ids.to(torch.int64), token_type_ids=None,labels=b_labels.to(torch.int64))\n",
    "        train_loss_set.append(loss.item())    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Update variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,dataLoader):\n",
    "    model.eval()\n",
    "    eval_accuracy = 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in dataLoader:\n",
    "        # Unpack the inputs from our dataloader\n",
    "        #b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids, b_labels = batch\n",
    "        # do not compute or store gradients\n",
    "        with torch.no_grad():\n",
    "            # calculate logit predictions\n",
    "            logits = model(b_input_ids.to(torch.int64), token_type_ids=None)\n",
    "        tmp_eval_accuracy = accuracy(logits, b_labels.to(torch.int64))\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return torch.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('D:/RiskProject/RCATestDataForBERT.csv',encoding='ISO-8859-1', usecols=['RISK DESCRIPTION','ORT LEVEL 1','ORT LEVEL 2'])\n",
    "df_mapData = pd.read_excel(\"D:/RiskProject/RiskCategories.xlsx\",usecols = ['ORT Level 1','ORT Level 2'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = encodeValues(LabelEncoder(),df_mapData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORT_LEVEL_1_DICT,ORT_LEVEL_2_DICT = createDictFunc(df_mapData,df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encoderFunc(df,ORT_LEVEL_1_DICT,ORT_LEVEL_2_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RISK DESCRIPTION'] = [\"[CLS] \" + desc + \" [SEP]\" for desc in df['RISK DESCRIPTION'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels_L1 = len(df['ORT LEVEL 1'].unique())\n",
    "num_labels_L20 = len(df['ORT LEVEL 2'].where(df['ORT LEVEL 1'] == 0).dropna().unique())\n",
    "num_labels_L21 = len(df['ORT LEVEL 2'].where(df['ORT LEVEL 1'] == 1).dropna().unique())\n",
    "num_labels_L22 = len(df['ORT LEVEL 2'].where(df['ORT LEVEL 1'] == 2).dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('D:/Bert_Models/bert-base-uncased')\n",
    "df['RISK DESCRIPTION'] = [tokenizer.tokenize(description) for description in df['RISK DESCRIPTION'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "df['RISK DESCRIPTION'] = [tokenizer.convert_tokens_to_ids(x) for x in df['RISK DESCRIPTION'].values]\n",
    "df['RISK DESCRIPTION'] = pad_sequences(df['RISK DESCRIPTION'].values,maxlen=max_length,dtype='long', truncating='post',padding='post').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BertForSequenceClassification.from_pretrained(\"D:/Bert_Models/bert-base-uncased\", num_labels=num_labels_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model1.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1010 07:38:13.629604  7204 optimization.py:46] t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L1, validation_inputs_L1, train_labels_L1, validation_labels_L1 = train_test_split(df['RISK DESCRIPTION'].values, df['ORT LEVEL 1'].values, test_size=0.1,random_state = 42,shuffle = True, stratify = df['ORT LEVEL 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L1 = torch.tensor(list(train_inputs_L1))\n",
    "validation_inputs_L1 = torch.tensor(list(validation_inputs_L1))\n",
    "train_labels_L1 = torch.tensor(list(train_labels_L1))\n",
    "validation_labels_L1 = torch.tensor(list(validation_labels_L1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_L1 = TensorDataset(train_inputs_L1, train_labels_L1)\n",
    "train_sampler_L1 = RandomSampler(train_data_L1)\n",
    "train_dataloader_L1 = DataLoader(train_data_L1, sampler=train_sampler_L1, batch_size=batch_size)\n",
    "\n",
    "validation_data_L1 = TensorDataset(validation_inputs_L1, validation_labels_L1)\n",
    "validation_sampler_L1 = SequentialSampler(validation_data_L1)\n",
    "validation_dataloader_L1 = DataLoader(validation_data_L1, sampler=validation_sampler_L1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "training(model1,train_dataloader_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model1,validation_dataloader_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROUP THE DATA BY ORT LEVEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.groupby('ORT LEVEL 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_0 = grouped_data.get_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_1 = grouped_data.get_group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_2 = grouped_data.get_group(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFY ORT LEVEL 2 for ORT LEVEL 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model20 = BertForSequenceClassification.from_pretrained(\"D:/Bert_Models/bert-base-uncased\", num_labels=num_labels_L20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model20.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L20, validation_inputs_L20, train_labels_L20, validation_labels_L20 = train_test_split(df_data_0['RISK DESCRIPTION'].values, df_data_0['ORT LEVEL 2'].values, test_size=0.1,random_state = 42,shuffle = True, stratify = df_data_0['ORT LEVEL 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L20 = torch.tensor(list(train_inputs_L20))\n",
    "validation_inputs_L20 = torch.tensor(list(validation_inputs_L20))\n",
    "train_labels_L20 = torch.tensor(list(train_labels_L20))\n",
    "validation_labels_L20 = torch.tensor(list(validation_labels_L20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_L20 = TensorDataset(train_inputs_L20, train_labels_L20)\n",
    "train_sampler_L20 = RandomSampler(train_data_L20)\n",
    "train_dataloader_L20 = DataLoader(train_data_L20, sampler=train_sampler_L20, batch_size=batch_size)\n",
    "\n",
    "validation_data_L20 = TensorDataset(validation_inputs_L20, validation_labels_L20)\n",
    "validation_sampler_L20 = SequentialSampler(validation_data_L20)\n",
    "validation_dataloader_L20 = DataLoader(validation_data_L20, sampler=validation_sampler_L20, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(model20,train_dataloader_L20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model20,validation_dataloader_L20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFY ORT LEVEL 2 for ORT LEVEL 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model21 = BertForSequenceClassification.from_pretrained(\"D:/Bert_Models/bert-base-uncased\", num_labels=num_labels_L21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model21.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L21, validation_inputs_L21, train_labels_L21, validation_labels_L21 = train_test_split(df_data_1['RISK DESCRIPTION'].values, df_data_1['ORT LEVEL 2'].values, test_size=0.1,random_state = 42,shuffle = True, stratify = df_data_1['ORT LEVEL 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L21 = torch.tensor(list(train_inputs_L21))\n",
    "validation_inputs_L21 = torch.tensor(list(validation_inputs_L21))\n",
    "train_labels_L21 = torch.tensor(list(train_labels_L21))\n",
    "validation_labels_L21 = torch.tensor(list(validation_labels_L21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_L21 = TensorDataset(train_inputs_L21, train_labels_L21)\n",
    "train_sampler_L21 = RandomSampler(train_data_L21)\n",
    "train_dataloader_L21 = DataLoader(train_data_L21, sampler=train_sampler_L21, batch_size=batch_size)\n",
    "\n",
    "validation_data_L21 = TensorDataset(validation_inputs_L21, validation_labels_L21)\n",
    "validation_sampler_L21 = SequentialSampler(validation_data_L21)\n",
    "validation_dataloader_L21 = DataLoader(validation_data_L21, sampler=validation_sampler_L21, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(model20,train_dataloader_L20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model20,validation_dataloader_L20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFY ORT LEVEL 2 for ORT LEVEL 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model22 = BertForSequenceClassification.from_pretrained(\"D:/Bert_Models/bert-base-uncased\", num_labels=num_labels_L22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model22.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L22, validation_inputs_L22, train_labels_L22, validation_labels_L22 = train_test_split(df_data_2['RISK DESCRIPTION'].values, df_data_2['ORT LEVEL 2'].values, test_size=0.1,random_state = 42,shuffle = True, stratify = df_data_2['ORT LEVEL 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_L22 = torch.tensor(list(train_inputs_L22))\n",
    "validation_inputs_L22 = torch.tensor(list(validation_inputs_L22))\n",
    "train_labels_L22 = torch.tensor(list(train_labels_L22))\n",
    "validation_labels_L22 = torch.tensor(list(validation_labels_L22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_L22 = TensorDataset(train_inputs_L22, train_labels_L22)\n",
    "train_sampler_L22 = RandomSampler(train_data_L22)\n",
    "train_dataloader_L22 = DataLoader(train_data_L22, sampler=train_sampler_L22, batch_size=batch_size)\n",
    "\n",
    "validation_data_L22 = TensorDataset(validation_inputs_L22, validation_labels_L22)\n",
    "validation_sampler_L22 = SequentialSampler(validation_data_L22)\n",
    "validation_dataloader_L22 = DataLoader(validation_data_L22, sampler=validation_sampler_L22, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(model20,train_dataloader_L20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model20,validation_dataloader_L20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single input test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('F:/unique_desc/test_description.csv') as fin:\n",
    "    description_test = fin.readline()\n",
    "description_test = \"[CLS] \" + description_test + \" [SEP]\"\n",
    "print(description_test)\n",
    "tokenized_test_text = tokenizer.tokenize(description_test)\n",
    "print(tokenized_test_text)\n",
    "test_input_ids = tokenizer.convert_tokens_to_ids(tokenized_test_text)\n",
    "print(test_input_ids)\n",
    "test_input_ids = pad_sequences([test_input_ids],maxlen = max_length,dtype='long', truncating='post', padding='post')\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "predictionsL1 = model1(test_input_ids.to(torch.int64))\n",
    "predicted_class_L1 = torch.argmax(predictionsL1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_L1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
