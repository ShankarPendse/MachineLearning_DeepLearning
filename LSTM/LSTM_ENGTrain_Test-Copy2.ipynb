{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import more_itertools as mit\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the seed value, so that the resutls can be reproduced\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Setting max length of a sequence\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Training batch size\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "\n",
    "# Number of epochs for training the model\n",
    "EPOCHS = 50\n",
    "\n",
    "# Path where we want to save the model\n",
    "MODEL_PATH = \"model_LSTM_ENG.bin\"\n",
    "\n",
    "# Train and Test files\n",
    "files = {\n",
    "        \"train\": \"./data/engtrain.bio\", \n",
    "        \"test\": \"./data/engtest.bio\"\n",
    "        }\n",
    "\n",
    "# Dimension of word embeddings\n",
    "EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to prepare the vocabulary using both train and test data set\n",
    "def prepare_vocab(files):\n",
    "    train_data = pd.read_csv(files['train'], sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=True, dtype=\"string\", skipfooter=1)\n",
    "    test_data = pd.read_csv(files['test'], sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=True, dtype=\"string\", skipfooter=1)\n",
    "    \n",
    "    train_vocab = set(train_data['WORD'])\n",
    "    test_vocab = set(test_data['WORD'])\n",
    "    \n",
    "    vocab = list(train_vocab.union(test_vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the dataset, pre process and prepare it for the model to consume for training\n",
    "\n",
    "def read_process_and_prepare_data(files):\n",
    "    \n",
    "    # Getting the vocabulary\n",
    "    vocab = prepare_vocab(files)\n",
    "    \n",
    "    # Constructing word to id and id to word dictionaries\n",
    "    word2id_dict = dict((word, idx) for idx, word in enumerate(vocab))\n",
    "    id2word_dict = {idx: word for word, idx in word2id_dict.items()}\n",
    "    \n",
    "    # Read the train dataset including blank lines (separator for each sentence)\n",
    "    df = pd.read_csv(files['train'], sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=False, dtype=\"string\", skipfooter=1)\n",
    "    \n",
    "    # Filling the blank lines which are read as null values in dataframe with a value \"split_at\"\n",
    "    df.fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    filt = df['TAG']!=\"split_at\"\n",
    "    tags_enc = LabelEncoder()\n",
    "    \n",
    "    # train the LabelEncoder on the 'TAG' column excluding the value \"split_at\"\n",
    "    tags_enc.fit_transform(df.loc[filt, 'TAG'])\n",
    "    \n",
    "    # Construct the tags2id dictionary: ({\"tag\": <id>})\n",
    "    tags2id_dict = dict(zip(tags_enc.classes_, tags_enc.transform(tags_enc.classes_)))\n",
    "    \n",
    "    # Replace the 'TAG' values with encoded values using dictionary constructed in above step\n",
    "    df['TAG'] = df['TAG'].map(tags2id_dict)\n",
    "    \n",
    "    # \"split_at\" will be ignored while replacing the 'TAG' values, as we did not want the labelEncoder to encode this special token\n",
    "    df['TAG'].fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    # Replace each word in 'WORD' column using the word2id dictionary constructed previously ({\"word\": <id>}) using vocabulary\n",
    "    df['WORD'] = df['WORD'].map(word2id_dict)\n",
    "    \n",
    "    # Since we did not include \"split_at\" in the vocabulary, while replacing words with id values, this will be skipped and will have NaN values\n",
    "    # Replacing NaN values with \"split_at\"\n",
    "    df['WORD'].fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    # Constructing sentences from 'WORD' column splitting at \"spli_at\" token (because sentences are delimited with \"split_at\" token)\n",
    "    sentences = np.array(list(mit.split_at(df['WORD'].tolist(), pred=lambda x: x=='split_at')))\n",
    "    \n",
    "    # For each sentence, construct list of tags for each word in that sentence\n",
    "    tags = list(list(mit.split_at(df['TAG'], pred=lambda x: x=='split_at')))\n",
    "    \n",
    "    # Pad the tags to MAX_LEN with value -1\n",
    "    padded_tags = pad_sequences(tags, maxlen=MAX_LEN, padding='post', value=-1)\n",
    "    \n",
    "    # Convert the tag for each word into onehot encoded vecotrs\n",
    "    padded_tags = to_categorical(padded_tags)\n",
    "    \n",
    "    \n",
    "    padding_value = len(vocab) + 1\n",
    "    \n",
    "    # Pad the sentences to MAX_LEN with the padding value as len(vocab)+1\n",
    "    padded_sentences = pad_sequences(sentences, maxlen=MAX_LEN, padding='post', truncating='post', value=padding_value)\n",
    "    \n",
    "    # Construct a dictionary and return for later use\n",
    "    dataset = {\n",
    "                \"sentences\": sentences,\n",
    "                \"padded_sentences\": padded_sentences,\n",
    "                \"tags\": padded_tags,\n",
    "                \"tags_enc\": tags_enc,\n",
    "                \"word2id\": word2id_dict,\n",
    "                \"id2word\": id2word_dict,\n",
    "                \"tags2id\": tags2id_dict,\n",
    "                \"vocab_len\": len(vocab)\n",
    "              }\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model\n",
    "\n",
    "def create_model(input_dim, output_dim, seq_len, num_tags):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=seq_len))\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True)))\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True))\n",
    "    model.add(Dense(num_tags, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = Adam(learning_rate = 0.001)\n",
    "    \n",
    "    # Using \"categorical_crossentropy\" loss as we are using one hot encoded representation of each tag\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # Print the summar of the model constructed above and return the compiled model\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the network\n",
    "\n",
    "def train_network(model, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    # Using EarlyStopping as callbacks while calling fit method on the model as we monitor the loss on validation set.\n",
    "    # This will prevent us from training for large number of epochs if there is no improvement in the validation loss\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Training the model and saving the history of the model training information\n",
    "    history = model.fit(x=X_train, y=y_train, batch_size = TRAIN_BATCH_SIZE, validation_data=(X_val, y_val), epochs = EPOCHS, callbacks=early_stopping)\n",
    "    \n",
    "    # Return the trained model and the history of it\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and prepare the test dataset\n",
    "\n",
    "def prepare_test_data(file, tag_enc, tags2id_dict, word2id_dict, vocab_len):\n",
    "    \n",
    "    # Read the dataset including the blan line separators and fill the null values with token \"split_at\"\n",
    "    df = pd.read_csv(file, sep=\"\\t\", names=['TAG', 'WORD'], skip_blank_lines=False, dtype=\"string\", skipfooter=1)\n",
    "    df.fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    filt = df['TAG']!=\"split_at\"\n",
    "    \n",
    "    # Using the encoder that is fit on train data to transform the tag values in test data\n",
    "    tag_enc.transform(df.loc[filt, 'TAG'])\n",
    "#     tags2id_dict = dict(zip(tags_enc.classes_, tags_enc.transform(tags_enc.classes_)))\n",
    "    \n",
    "    df['TAG'] = df['TAG'].map(tags2id_dict)\n",
    "    \n",
    "    # \"split_at\" token will be left out during encoding the tag values and will be replaced as NaN, so replacing NaN with \"split_at\"\n",
    "    df['TAG'].fillna(\"split_at\", inplace=True)\n",
    "    \n",
    "    # Using the vocab mapping dictionary of word to id, convert the words into their respective ids\n",
    "    df['WORD'] = df['WORD'].map(word2id_dict)\n",
    "    \n",
    "    # Fill the NaN VALUES FOR WORD colum with \"split_at\"\n",
    "    df['WORD'].fillna(\"split_at\", inplace=True)\n",
    "        \n",
    "    # Construct the sentences with the help of \"split_at\" token using more_itertools\n",
    "    sentences = np.array(list(mit.split_at(df['WORD'].tolist(), pred=lambda x: x=='split_at')))\n",
    "    \n",
    "    # For each sentence, get the list of tags for each word in that sentence\n",
    "    tags = list(list(mit.split_at(df['TAG'], pred=lambda x: x=='split_at')))\n",
    "    \n",
    "    # Pad the tags for each sentence to have MAX_LEN\n",
    "    padded_tags = pad_sequences(tags, maxlen=MAX_LEN, padding='post', value=-1)\n",
    "    \n",
    "    # Convert the tags to one hot encoded representation\n",
    "    padded_tags = to_categorical(padded_tags)\n",
    "    \n",
    "    padding_value = vocab_len\n",
    "    \n",
    "    # Pad each sentences to have MAX_LEN\n",
    "    padded_sentences = pad_sequences(sentences, maxlen=MAX_LEN, padding='post', truncating='post', value=padding_value)\n",
    "    \n",
    "    # Return the dictionary for later usage\n",
    "    test_dataset = {\n",
    "                    \"sentences\": sentences,\n",
    "                    \"padded_sentences\": padded_sentences,\n",
    "                    \"tags\": tags,\n",
    "                    \"padded_tags\": padded_tags\n",
    "                    }\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  del sys.path[0]\n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:41: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 64, 256)           1915648   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64, 512)           1050624   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 256)           787456    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64, 25)            6425      \n",
      "=================================================================\n",
      "Total params: 3,760,153\n",
      "Trainable params: 3,760,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "123/123 [==============================] - 7s 58ms/step - loss: 0.3705 - accuracy: 0.9308 - val_loss: 0.2553 - val_accuracy: 0.9388\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 7s 54ms/step - loss: 0.1888 - accuracy: 0.9496 - val_loss: 0.1473 - val_accuracy: 0.9598\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 7s 57ms/step - loss: 0.1130 - accuracy: 0.9702 - val_loss: 0.1031 - val_accuracy: 0.9731\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - 7s 56ms/step - loss: 0.0749 - accuracy: 0.9814 - val_loss: 0.0804 - val_accuracy: 0.9808\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 7s 59ms/step - loss: 0.0540 - accuracy: 0.9874 - val_loss: 0.0698 - val_accuracy: 0.9831\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 7s 58ms/step - loss: 0.0414 - accuracy: 0.9902 - val_loss: 0.0672 - val_accuracy: 0.9837\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 7s 57ms/step - loss: 0.0346 - accuracy: 0.9916 - val_loss: 0.0620 - val_accuracy: 0.9852\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 7s 55ms/step - loss: 0.0292 - accuracy: 0.9928 - val_loss: 0.0629 - val_accuracy: 0.9846\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 7s 53ms/step - loss: 0.0253 - accuracy: 0.9937 - val_loss: 0.0628 - val_accuracy: 0.9850\n",
      "Epoch 10/50\n",
      "123/123 [==============================] - 7s 56ms/step - loss: 0.0221 - accuracy: 0.9945 - val_loss: 0.0633 - val_accuracy: 0.9853\n",
      "Epoch 11/50\n",
      "123/123 [==============================] - 7s 58ms/step - loss: 0.0191 - accuracy: 0.9951 - val_loss: 0.0669 - val_accuracy: 0.9843\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 7s 57ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 0.0656 - val_accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read the data\n",
    "    dataset = read_process_and_prepare_data(files)\n",
    "    \n",
    "    # Split the training data into train and val data with 10% of train data set aside as validation data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(dataset['padded_sentences'], np.array([np.array(x) for x in dataset['tags']]), random_state=42, test_size=0.2)\n",
    "    \n",
    "    # Creating the model\n",
    "    model = create_model(input_dim=dataset['vocab_len'] + 2, output_dim=EMBEDDING_DIM, seq_len=MAX_LEN, num_tags=len(dataset['tags_enc'].classes_))\n",
    "    \n",
    "    # Calling the function to train the network (model)\n",
    "    history, model = train_network(model, np.array(X_train), np.array(y_train), np.array(X_val), np.array(y_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.92      0.88       812\n",
      "         1.0       0.28      0.10      0.15        90\n",
      "         2.0       0.88      0.78      0.83       456\n",
      "         3.0       0.89      0.93      0.91      1117\n",
      "         4.0       0.55      0.60      0.57       491\n",
      "         5.0       0.98      0.97      0.97       500\n",
      "         6.0       0.91      0.74      0.82       451\n",
      "         7.0       0.00      0.00      0.00        56\n",
      "         8.0       0.84      0.39      0.53        54\n",
      "         9.0       0.77      0.33      0.46       562\n",
      "        10.0       0.00      0.00      0.00        30\n",
      "        11.0       0.96      0.93      0.94       720\n",
      "        12.0       0.82      0.93      0.87       862\n",
      "        13.0       0.43      0.08      0.13        75\n",
      "        14.0       0.92      0.77      0.84       496\n",
      "        15.0       0.87      0.68      0.76       222\n",
      "        16.0       0.63      0.36      0.46       496\n",
      "        17.0       0.93      0.87      0.90       226\n",
      "        18.0       0.85      0.71      0.77       403\n",
      "        19.0       0.00      0.00      0.00        45\n",
      "        20.0       0.84      0.47      0.60       119\n",
      "        21.0       0.85      0.37      0.51       856\n",
      "        22.0       0.00      0.00      0.00         8\n",
      "        23.0       0.96      0.97      0.96       610\n",
      "        24.0       0.90      0.98      0.94     14929\n",
      "\n",
      "    accuracy                           0.88     24686\n",
      "   macro avg       0.68      0.55      0.59     24686\n",
      "weighted avg       0.87      0.88      0.87     24686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Check the trained model performance on the test dataset\n",
    "\n",
    "# Read and prepare the test dataset\n",
    "test_dataset = prepare_test_data(file=files['test'], tag_enc=dataset['tags_enc'], tags2id_dict=dataset['tags2id'], word2id_dict=dataset['word2id'], vocab_len=dataset['vocab_len'])\n",
    "\n",
    "# Predict the tags for the teset dataset using the trained model\n",
    "test_predictions = model.predict(test_dataset['padded_sentences'])\n",
    "\n",
    "# Get the predictions of tag for each word in the sentence for each sentence\n",
    "predicted_tags = []\n",
    "for i in range(len(test_dataset['tags'])):\n",
    "    predicted_tags.append( np.argmax(test_predictions[i], axis=1)[:len(test_dataset['sentences'][i])] )\n",
    "\n",
    "# Get the classification report to check the model performance\n",
    "print(classification_report(np.concatenate(test_dataset['tags']), np.concatenate(predicted_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance on Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.96      0.91      3220\n",
      "         1.0       0.45      0.21      0.29       385\n",
      "         2.0       0.93      0.91      0.92      1720\n",
      "         3.0       0.93      0.95      0.94      4354\n",
      "         4.0       0.74      0.75      0.75      1927\n",
      "         5.0       0.98      0.97      0.97      2007\n",
      "         6.0       0.90      0.76      0.83      1869\n",
      "         7.0       0.00      0.00      0.00       221\n",
      "         8.0       0.88      0.40      0.55       245\n",
      "         9.0       0.92      0.44      0.60      2376\n",
      "        10.0       0.00      0.00      0.00       113\n",
      "        11.0       0.97      0.94      0.95      2858\n",
      "        12.0       0.87      0.96      0.91      3474\n",
      "        13.0       0.55      0.11      0.18       342\n",
      "        14.0       0.96      0.93      0.95      1850\n",
      "        15.0       0.94      0.80      0.86       786\n",
      "        16.0       0.85      0.57      0.68      1687\n",
      "        17.0       0.98      0.83      0.90       840\n",
      "        18.0       0.89      0.77      0.83      1673\n",
      "        19.0       0.00      0.00      0.00       132\n",
      "        20.0       0.92      0.60      0.73       446\n",
      "        21.0       0.98      0.54      0.69      3495\n",
      "        22.0       0.00      0.00      0.00         7\n",
      "        23.0       0.98      0.96      0.97      2456\n",
      "        24.0       0.92      0.99      0.95     61008\n",
      "\n",
      "    accuracy                           0.92     99491\n",
      "   macro avg       0.74      0.61      0.65     99491\n",
      "weighted avg       0.91      0.92      0.91     99491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Check the trained model performance on the train dataset\n",
    "\n",
    "# Read and prepare the test dataset\n",
    "dataset = prepare_test_data(file=files['train'], tag_enc=dataset['tags_enc'], tags2id_dict=dataset['tags2id'], word2id_dict=dataset['word2id'], vocab_len=dataset['vocab_len'])\n",
    "\n",
    "# Predict the tags for the teset dataset using the trained model\n",
    "train_predictions = model.predict(dataset['padded_sentences'])\n",
    "\n",
    "# Get the predictions of tag for each word in the sentence for each sentence\n",
    "predicted_tags = []\n",
    "for i in range(len(dataset['tags'])):\n",
    "    predicted_tags.append(np.argmax(train_predictions[i], axis=1)[:len(dataset['sentences'][i])])\n",
    "\n",
    "# Get the classification report to check the model performance\n",
    "print(classification_report(np.concatenate(dataset['tags']), np.concatenate(predicted_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
