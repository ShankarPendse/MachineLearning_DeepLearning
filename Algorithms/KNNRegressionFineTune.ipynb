{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from Part1A import calculate_distances, calculate_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mahalanobis as measure of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('Regression/trainingData.csv', delimiter = \",\")\n",
    "test_data = np.genfromtxt('Regression/testData.csv', delimiter = \",\")\n",
    "KNNRM = KNeighborsRegressor(n_neighbors=3, algorithm = 'brute', \n",
    "                            metric='mahalanobis', metric_params={'V':np.cov(train_data[:,0:-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='brute', metric='mahalanobis',\n",
       "                    metric_params={'V': array([[ 1.10627836,  0.12535803, -0.02765481, ..., -0.41880522,\n",
       "        -0.59713489,  0.20156931],\n",
       "       [ 0.12535803,  1.00190315, -0.1984574 , ...,  0.30053543,\n",
       "        -0.15101237,  0.22505434],\n",
       "       [-0.02765481, -0.1984574 ,  0.95482353, ..., -0.22128758,\n",
       "        -0.01273118,  0.23208212],\n",
       "       ...,\n",
       "       [-0.41880522,  0.30053543, -0.22128758, ...,  0.82752817,\n",
       "         0.36539651, -0.37336213],\n",
       "       [-0.59713489, -0.15101237, -0.01273118, ...,  0.36539651,\n",
       "         0.81258381,  0.0698001 ],\n",
       "       [ 0.20156931,  0.22505434,  0.23208212, ..., -0.37336213,\n",
       "         0.0698001 ,  1.01664977]])},\n",
       "                    n_neighbors=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNRM.fit(train_data[:,0:-1], train_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1462: FutureWarning: from version 0.25, pairwise_distances for metric='mahalanobis' will require VI to be specified if Y is passed.\n",
      "  \"specified if Y is passed.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8205332483357298"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNRM.score(test_data[:,0:-1], test_data[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Min-Max scaler (Normalization) and inverse of squared distance as weights\n",
    "## Implementing it using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('Regression/trainingData.csv', delimiter = \",\")\n",
    "test_data = np.genfromtxt('Regression/testData.csv', delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(training_features, query_features):\n",
    "    \n",
    "    # call calculate_distances\n",
    "    distances = calculate_distances(training_features, query_features)\n",
    "    \n",
    "    # Set the number of neighbours to consider\n",
    "    k = 3 # We can paramterize this so that while calling predict value of K can be passed as an argument to this function. Instead of 2 parameters this function will then take 3 parameters\n",
    "    \n",
    "    # use np.argsort to return indices from distances\n",
    "    indices = np.argsort(distances)\n",
    "    \n",
    "    # average the 3 nearest disances\n",
    "    #predicted_value = np.mean(train_data[indices[0:k],-1])\n",
    "    \n",
    "    # Inverse weighted distance average\n",
    "    #predicted_value = np.sum(train_data[indices[0:k],-1] * (1/(distances[indices[0:k]]))) / np.sum((1/(distances[indices[0:k]])))\n",
    "    \n",
    "    # squared inverse weighted distance average\n",
    "    predicted_value = np.sum(train_data[indices[0:k],-1] * np.square(1/(distances[indices[0:k]]))) / np.sum(np.square(1/(distances[indices[0:k]])))\n",
    "    \n",
    "    # return the prediction\n",
    "    return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score is:  0.8286449277052543\n"
     ]
    }
   ],
   "source": [
    "min_train_data = train_data[:,0:-1].min(axis = 0)\n",
    "max_train_data = train_data[:,0:-1].max(axis = 0)\n",
    "\n",
    "train_data[:,0:-1] = ( train_data[:,0:-1] - train_data[:,0:-1].min(axis = 0) ) / ( train_data[:,0:-1].max(axis = 0) - train_data[:,0:-1].min(axis = 0) )\n",
    "\n",
    "test_data[:,0:-1] = ( test_data[:, 0:-1] - min_train_data ) / ( max_train_data - min_train_data )\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    predicted_value = predict(train_data[:,0:-1], test_data[i,0:-1])\n",
    "    predictions.append(predicted_value)\n",
    "r2_score = calculate_r2(test_data[:,-1], np.array(predictions))\n",
    "print(\"R2 Score is: \", r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From above two methods, it is evident that the altering the scale of the features is not improve the R squared error by much.\n",
    "\n",
    "# Let's check the correlation among the features to see if there are any weak predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us convert the numpy array into pandas dataframe to get better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('Regression/trainingData.csv', delimiter = \",\")\n",
    "test_data = np.genfromtxt('Regression/testData.csv', delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.008373</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.022645</td>\n",
       "      <td>-0.000628</td>\n",
       "      <td>0.014265</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>-0.004684</td>\n",
       "      <td>-0.017824</td>\n",
       "      <td>0.004609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.020603</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.021844</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>-0.005987</td>\n",
       "      <td>-0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007072</td>\n",
       "      <td>-0.020603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.014672</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>-0.008521</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>-0.013469</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>-0.013291</td>\n",
       "      <td>-0.022593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000705</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>0.020930</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>-0.009665</td>\n",
       "      <td>-0.013561</td>\n",
       "      <td>0.014955</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.006303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008373</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>-0.014672</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018053</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>-0.018079</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>-0.007775</td>\n",
       "      <td>-0.002404</td>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>0.020930</td>\n",
       "      <td>0.018053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>-0.014863</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.009425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022645</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>-0.012111</td>\n",
       "      <td>-0.014598</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.500095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000628</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.008521</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015647</td>\n",
       "      <td>-0.008439</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>-0.010313</td>\n",
       "      <td>0.484448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.014265</td>\n",
       "      <td>-0.021844</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>-0.009665</td>\n",
       "      <td>-0.018079</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>-0.015647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>-0.005209</td>\n",
       "      <td>-0.027966</td>\n",
       "      <td>0.152783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>-0.013469</td>\n",
       "      <td>-0.013561</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>-0.014863</td>\n",
       "      <td>-0.012111</td>\n",
       "      <td>-0.008439</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013414</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.466152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.004684</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.014955</td>\n",
       "      <td>-0.007775</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>-0.014598</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>-0.005209</td>\n",
       "      <td>0.013414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004695</td>\n",
       "      <td>-0.000782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.017824</td>\n",
       "      <td>-0.005987</td>\n",
       "      <td>-0.013291</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.002404</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>-0.010313</td>\n",
       "      <td>-0.027966</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-0.004695</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.504330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.004609</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>-0.022593</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.500095</td>\n",
       "      <td>0.484448</td>\n",
       "      <td>0.152783</td>\n",
       "      <td>0.466152</td>\n",
       "      <td>-0.000782</td>\n",
       "      <td>0.504330</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000 -0.004362  0.007072  0.000705  0.008373 -0.011065  0.022645   \n",
       "1  -0.004362  1.000000 -0.020603 -0.001333 -0.005931  0.008843  0.008947   \n",
       "2   0.007072 -0.020603  1.000000  0.000111 -0.014672  0.014021 -0.010896   \n",
       "3   0.000705 -0.001333  0.000111  1.000000 -0.000592  0.020930  0.003464   \n",
       "4   0.008373 -0.005931 -0.014672 -0.000592  1.000000  0.018053  0.000553   \n",
       "5  -0.011065  0.008843  0.014021  0.020930  0.018053  1.000000  0.010928   \n",
       "6   0.022645  0.008947 -0.010896  0.003464  0.000553  0.010928  1.000000   \n",
       "7  -0.000628 -0.004167 -0.008521  0.018884  0.006604  0.009479  0.000829   \n",
       "8   0.014265 -0.021844  0.000466 -0.009665 -0.018079  0.009403  0.004241   \n",
       "9   0.003355  0.004767 -0.013469 -0.013561  0.001130 -0.014863 -0.012111   \n",
       "10 -0.004684  0.004464  0.002297  0.014955 -0.007775  0.013834 -0.014598   \n",
       "11 -0.017824 -0.005987 -0.013291  0.000111 -0.002404  0.009100  0.001990   \n",
       "12  0.004609 -0.001219 -0.022593  0.006303  0.000415  0.009425  0.500095   \n",
       "\n",
       "          7         8         9         10        11        12  \n",
       "0  -0.000628  0.014265  0.003355 -0.004684 -0.017824  0.004609  \n",
       "1  -0.004167 -0.021844  0.004767  0.004464 -0.005987 -0.001219  \n",
       "2  -0.008521  0.000466 -0.013469  0.002297 -0.013291 -0.022593  \n",
       "3   0.018884 -0.009665 -0.013561  0.014955  0.000111  0.006303  \n",
       "4   0.006604 -0.018079  0.001130 -0.007775 -0.002404  0.000415  \n",
       "5   0.009479  0.009403 -0.014863  0.013834  0.009100  0.009425  \n",
       "6   0.000829  0.004241 -0.012111 -0.014598  0.001990  0.500095  \n",
       "7   1.000000 -0.015647 -0.008439  0.006654 -0.010313  0.484448  \n",
       "8  -0.015647  1.000000  0.016315 -0.005209 -0.027966  0.152783  \n",
       "9  -0.008439  0.016315  1.000000  0.013414  0.000968  0.466152  \n",
       "10  0.006654 -0.005209  0.013414  1.000000 -0.004695 -0.000782  \n",
       "11 -0.010313 -0.027966  0.000968 -0.004695  1.000000  0.504330  \n",
       "12  0.484448  0.152783  0.466152 -0.000782  0.504330  1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us check the correlation\n",
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above correlation table represents the correlation among the features.\n",
    "\n",
    "### We can see that the target variable (represented by the row number and column number 12) is having very weak correlation with the feature variables (numbered 0,1,3,4,5, and 10) which are not contributing much to the value of our target variable. Let us remove those features from our data set and check if our R2 Score improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_data = train_data[:,[2,6,7,8,9,11,12]]\n",
    "# new_test_data = test_data[:,[2,6,7,8,9,11,12]]\n",
    "\n",
    "new_train_data = train_data[:,[6,7,8,9,11,12]]\n",
    "new_test_data = test_data[:,[6,7,8,9,11,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(training_features, query_features):\n",
    "    \n",
    "    # call calculate_distances\n",
    "    distances = calculate_distances(training_features, query_features)\n",
    "    \n",
    "    # Set the number of neighbours to consider\n",
    "    k = 3 # We can paramterize this so that while calling predict value of K can be passed as an argument to this function. Instead of 2 parameters this function will then take 3 parameters\n",
    "    \n",
    "    # use np.argsort to return indices from distances\n",
    "    indices = np.argsort(distances)\n",
    "    \n",
    "    # average the 3 nearest disances\n",
    "    #predicted_value = np.mean(train_data[indices[0:k],-1])\n",
    "    \n",
    "    # Inverse weighted distance average\n",
    "    #predicted_value = np.sum(train_data[indices[0:k],-1] * (1/(distances[indices[0:k]]))) / np.sum((1/(distances[indices[0:k]])))\n",
    "    \n",
    "    # squared inverse weighted distance average\n",
    "    predicted_value = np.sum(new_train_data[indices[0:k],-1] * np.square(1/(distances[indices[0:k]]))) / np.sum(np.square(1/(distances[indices[0:k]])))\n",
    "    \n",
    "    # return the prediction\n",
    "    return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score is:  0.9611635779734792\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(len(new_test_data)):\n",
    "    predicted_value = predict(new_train_data[:,0:-1], new_test_data[i,0:-1])\n",
    "    predictions.append(predicted_value)\n",
    "r2_score = calculate_r2(new_test_data[:,-1], np.array(predictions))\n",
    "print(\"R2 Score is: \", r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the weak predictors which were pulling down the performance of KMeans algorith, has improved the score significantly.. Let us now check the R2 Score for different values of K (no of neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(training_features, query_features, k):\n",
    "    \n",
    "    # call calculate_distances\n",
    "    distances = calculate_distances(training_features, query_features)\n",
    "    \n",
    "    # Set the number of neighbours to consider\n",
    "    #k = 2 # We can paramterize this so that while calling predict value of K can be passed as an argument to this function. Instead of 2 parameters this function will then take 3 parameters\n",
    "    \n",
    "    # use np.argsort to return indices from distances\n",
    "    indices = np.argsort(distances)\n",
    "    \n",
    "    # average the 3 nearest disances\n",
    "    #predicted_value = np.mean(train_data[indices[0:k],-1])\n",
    "    \n",
    "    # Inverse weighted distance average\n",
    "    #predicted_value = np.sum(train_data[indices[0:k],-1] * (1/(distances[indices[0:k]]))) / np.sum((1/(distances[indices[0:k]])))\n",
    "    \n",
    "    # squared inverse weighted distance average\n",
    "    predicted_value = np.sum(new_train_data[indices[0:k],-1] * np.square(1/(distances[indices[0:k]]))) / np.sum(np.square(1/(distances[indices[0:k]])))\n",
    "    \n",
    "    # return the prediction\n",
    "    return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1, R2_Score = 0.9286765460737724\n",
      "K = 2, R2_Score = 0.9534594790307207\n",
      "K = 3, R2_Score = 0.9611635779734792\n",
      "K = 4, R2_Score = 0.9660481252882993\n",
      "K = 5, R2_Score = 0.9682009605433861\n",
      "K = 6, R2_Score = 0.9682937694255859\n",
      "K = 7, R2_Score = 0.9692738087759513\n",
      "K = 8, R2_Score = 0.9692388281783086\n",
      "K = 9, R2_Score = 0.9694553039757106\n",
      "K = 10, R2_Score = 0.9695399682791874\n"
     ]
    }
   ],
   "source": [
    "K = [1,2,3,4,5,6,7,8,9,10]\n",
    "r2_scores = []\n",
    "for k in K:\n",
    "    predictions = []\n",
    "    for i in range(len(new_test_data)):\n",
    "        predicted_value = predict(new_train_data[:,0:-1], new_test_data[i,0:-1], k)\n",
    "        predictions.append(predicted_value)\n",
    "    r2_score = calculate_r2(new_test_data[:,-1], np.array(predictions))\n",
    "    r2_scores.append(r2_score)\n",
    "    print(\"K = {}, R2_Score = {}\".format(k, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(K, r2_scores)\n",
    "plt.title(\"R2_Score vs K values\")\n",
    "plt.xlabel(\"K (Neighbours)\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From above plot, we can see that the R2_score has increased by 1% from K = 2 to K = 4, there after we do not see much improvement in our R2_score. We can choose K = 4 (Neighbours) for this problem as the best number of nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mahalanobis on selected Strong predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('Regression/trainingData.csv', delimiter = \",\")\n",
    "test_data = np.genfromtxt('Regression/testData.csv', delimiter = \",\")\n",
    "KNNRM = KNeighborsRegressor(n_neighbors=3, algorithm = 'brute', \n",
    "                            metric='mahalanobis', metric_params={'V':np.cov(new_train_data[:,0:-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='brute', metric='mahalanobis',\n",
       "                    metric_params={'V': array([[ 0.0053048 , -0.03783644, -0.05894303, ..., -0.05608456,\n",
       "        -0.00221903,  0.00551058],\n",
       "       [-0.03783644,  0.56439696,  0.44349119, ...,  0.24860501,\n",
       "        -0.33877491,  0.0638371 ],\n",
       "       [-0.05894303,  0.44349119,  0.79474251, ...,  0.39779431,\n",
       "        -0.27049391,  0.09141051],\n",
       "       ...,\n",
       "       [-0.05608456,  0.24860501,  0.39779431, ...,  1.01016537,\n",
       "         0.59786998, -0.38187748],\n",
       "       [-0.00221903, -0.33877491, -0.27049391, ...,  0.59786998,\n",
       "         0.99719971, -0.29464167],\n",
       "       [ 0.00551058,  0.0638371 ,  0.09141051, ..., -0.38187748,\n",
       "        -0.29464167,  0.43708408]])},\n",
       "                    n_neighbors=3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNRM.fit(new_train_data[:,0:-1], new_train_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1462: FutureWarning: from version 0.25, pairwise_distances for metric='mahalanobis' will require VI to be specified if Y is passed.\n",
      "  \"specified if Y is passed.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9606728074654239"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNRM.score(new_test_data[:,0:-1], new_test_data[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization on selected strong predictors and checking its effect on KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_train_data = new_train_data[:,0:-1].min(axis = 0)\n",
    "max_train_data = new_train_data[:,0:-1].max(axis = 0)\n",
    "\n",
    "new_train_data[:,0:-1] = ( new_train_data[:,0:-1] - new_train_data[:,0:-1].min(axis = 0) ) / ( new_train_data[:,0:-1].max(axis = 0) - new_train_data[:,0:-1].min(axis = 0) )\n",
    "\n",
    "new_test_data[:,0:-1] = ( new_test_data[:, 0:-1] - min_train_data ) / ( max_train_data - min_train_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1, R2_Score = 0.9272798660104137\n",
      "K = 2, R2_Score = 0.9533421526653921\n",
      "K = 3, R2_Score = 0.9605015448768033\n",
      "K = 4, R2_Score = 0.9651223592663756\n",
      "K = 5, R2_Score = 0.9665981111969462\n",
      "K = 6, R2_Score = 0.96776183686209\n",
      "K = 7, R2_Score = 0.9675790432065086\n",
      "K = 8, R2_Score = 0.9678005536359034\n",
      "K = 9, R2_Score = 0.9678328090476075\n",
      "K = 10, R2_Score = 0.967446664594377\n"
     ]
    }
   ],
   "source": [
    "for k in K:\n",
    "    predictions = []\n",
    "    for i in range(len(new_test_data)):\n",
    "        predicted_value = predict(new_train_data[:,0:-1], new_test_data[i,0:-1], k)\n",
    "        predictions.append(predicted_value)\n",
    "    r2_score = calculate_r2(new_test_data[:,-1], np.array(predictions))\n",
    "    r2_scores.append(r2_score)\n",
    "    print(\"K = {}, R2_Score = {}\".format(k, r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From above results it is safe to conclude that in the given data set, euclidean distance with inverse squared distance weights is performing much better, and that there was issue with existance of some unwanted / weak predictors which affected our model performance earlier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
